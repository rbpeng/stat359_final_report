# Final report for STAT 359 Data Science Project

## Summary 

During the course of this class, I worked mainly on the data acquisition and data ingestion portions of the project. At the beginning of the course, I worked on familiarizing myself with Python and using Python to acquire time series data from Alpha Vantage APIs, which can be found [here](https://github.com/rbpeng/stat359_project1). In particular, I wrote wrapper functions to obtain daily equity data, daily currency exchange rate data, and intraday currency exchange rate data, which takes produces csv files of the requested data. 

After working on this data acquisition project, I followed Professor Liu's advice and shifted to projects more aligned with the data ingestion and management portions of the data science pipeline. In particular, I focused on data lake infrastructures, which are centralized repositories that allows you to store all your data (both structured and unstructured) at any scale. In addition to doing research on data lakes, including setup of data lakes, cloud platforms to host data lake structures, organization of data lakes, limitations, and various other aspects, I also deployed a sample data lake infrastructure using AWS. I then explored various features of data lakes using guides from AWS, which allowed me to perform search, transforms, queries, analytics, and visualization of sample data provided by the AWS Data Lake demo. Using these guides, I was able to put together a working data lake through AWS and was able to give a walkthrough of the key features of the data lake infrastructure, using [AWS Lake Formation](https://aws.amazon.com/lake-formation/) to establish a centralized platform to manage and secure the data lake architecture.

The work I did on data acquisition and data lakes fits into the data science infrastructure framework by providing key steps in the data ingestion pipeline. A well-organized data ingestion pipeline is a key component of most data science projects (Recall data ingestion is the process by which data is moved from one or more sources to a destination where it can be stored and prepared for further analysis), since you can't do any data science projects without having data! It is vitally important to have a well-organized infrastructure for moving streaming data and batched data from pre-existing databases and data sources to a repository where you can store and prepare the data for further analysis. More and more commonly for many companies, this repository takes form in a data lake, which are particularly advantageous because they can hold data in its raw form (whatever that form may be), allowing us to accommodate any future schema requirements or design changes. In addition, data lakes are increasingly being deployed on the cloud rather than being hosted on-premises. Thus, it is important that we become familiarized with cloud-based data lake infrastructures, since if we do decide to utilize a data lake structure for our projects, such as PredicT, it will most likely be cloud-based. It is also important to become familiar with the features of a data lake infrastructure, as these features comprise key steps within the data ingestion pipeline. Thankfully, AWS Lake Formation provides a nice centralized platform to manage the data lake and data ingestion process, which is why I spent the last few weeks familiarizing myself with this tool. 

## Instructions & Guides

For the data acquisition through Alpha Vantage APIs, one can follow the [GitHub repository here](https://github.com/rbpeng/stat359_project1) for instructions to use the functions I wrote, as well as the code for the functions. For instance, `get_daily_equity_data(outdir="C:/Users/RBP7855/Desktop", filename="MSFT", stock='MSFT', values='high', num_days=500)` would produce a csv file named `MSFT.csv` in the specified directory, with 500 observations of the highest value of the equity `MSFT` each day. As another example, `get_daily_currency_data(outdir="C:/Users/RBP7855/Desktop", filename="JPY_to_USD", curr1='JPY', curr2='USD', values='low', num_days=1000)` would produce a csv file named `JPY_to_USD.csv` in the specified directory, with 1000 observations of the lowest currency exchange value from Japanese Yen to US Dollar each day. These functions are basically wrapper functions around default [Alpha Vantage functions](https://alpha-vantage.readthedocs.io/en/latest/) (from the Python package `alpha_vantage`). 

Here is an example of the csv file produced by the `get_daily_equity_data` function:

![Example](https://github.com/rbpeng/stat359_final_report/blob/master/MSFT%20example.png?raw=true)

I also did some other work with getting data from APIs with Python, and I used [this guide](https://www.dataquest.io/blog/python-api-tutorial/) for that. 

For my work with deploying a data lake infrastructure and going through the features of the data lake, I mainly used these two following guides from AWS:
[Quick Start Reference Demo](https://aws-quickstart.s3.amazonaws.com/quickstart-datalake-47lining/doc/data-lake-foundation-on-the-aws-cloud-with-aws-services.pdf)
[Demo and Walkthrough](https://aws-quickstart.s3.amazonaws.com/quickstart-demo-47lining-datalake-foundation/doc/data-lake-foundation-on-aws-demo-and-walkthrough.pdf)

The work I did with building a data lake involved less coding, so I will mainly provide screenshots so that you can follow along with what I did. The first guide above is what I used to build a new AWS environment consisting of the virtual private cloud (VPC), subnets, NAT gateways, bastion hosts, security groups, and other infrastructure components, and then deploy the data lake services and components into this new VPC. The Quick Start basically builds a data lake foundation that integrates AWS services such as Amazon S3, Amazon Redshift, Amazon Kinesis, Amazon Athena, AWS Glue, Amazon Elasticsearch Service (Amazon ES), Amazon SageMaker, and Amazon
QuickSight to provide data lake features, such as ingest processing, management, search, transforms, queries, analytics, and visualization. 

After logging into your AWS account and creating a key pair for the region you select (follow [this guide](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-key-pairs.html) to create your key pair), you can deploy the Quick Start data lake into a new VPC on AWS using [this link](https://fwd.aws/7D5gP). I kept all the default settings while deploying the Quick Start, but you will need to provide certain required inputs (such as a password for Amazon Redshift and at least two availability regions, as well as a trusted IP address for Remote Access CIDR). Once you provide all of these inputs, click on "Create stack" on the "Review" page, as seen here:
![Deploy Quick Start](https://github.com/rbpeng/stat359_final_report/blob/master/Deploy%20Quick%20Start.PNG?raw=true)
